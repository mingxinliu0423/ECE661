{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhDUFBxt9xZg"
      },
      "source": [
        "# Implement and train a LSTM for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6ymxu99xZk"
      },
      "source": [
        "## Step 0: set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spc_UH4B9xZl",
        "outputId": "3800827c-d1e7-48ed-ae59-2531e3933c2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kojil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "os.makedirs(\"resources\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEK3DN9Fwh_M"
      },
      "source": [
        "### Hyperparameters. No need to touch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OxnFjs3f9xZn"
      },
      "outputs": [],
      "source": [
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        # Constance hyperparameters. They have been tested and don't need to be tuned.\n",
        "        self.PAD_INDEX = 0\n",
        "        self.UNK_INDEX = 1\n",
        "        self.PAD_TOKEN = '<pad>'\n",
        "        self.UNK_TOKEN = '<unk>'\n",
        "        self.STOP_WORDS = set(stopwords.words('english'))\n",
        "        self.MAX_LENGTH = 256\n",
        "        self.BATCH_SIZE = 96\n",
        "        self.EMBEDDING_DIM = 1\n",
        "        self.HIDDEN_DIM = 100\n",
        "        self.OUTPUT_DIM = 2\n",
        "        self.N_LAYERS = 1\n",
        "        self.DROPOUT_RATE = 0.0\n",
        "        self.LR = 0.001\n",
        "        self.N_EPOCHS = 5\n",
        "        self.WD = 0\n",
        "        self.SEED = 12\n",
        "        self.BIDIRECTIONAL = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODz_aDV9xZo"
      },
      "source": [
        "## Lab 1(a) Implement your own data loader function.  \n",
        "First, you need to read the data from the dataset file on the local disk.\n",
        "Then, split the dataset into three sets: train, validation and test by 7:1:2 ratio.\n",
        "Finally return x_train, x_valid, x_test, y_train, y_valid, y_test where x represents reviews and y represent labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AD7HSvM19xZp"
      },
      "outputs": [],
      "source": [
        "def load_imdb(base_csv:str = './IMDBDataset.csv'):\n",
        "    \"\"\"\n",
        "    Load the IMDB dataset\n",
        "    :param base_csv: the path of the dataset file.\n",
        "    :return: train, validation and test set.\n",
        "    \"\"\"\n",
        "    # Add your code here.\n",
        "    df = pd.read_csv(base_csv)\n",
        "    X, y = df['review'], df['sentiment']\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2,\n",
        "    )\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "        x_train, y_train, test_size=0.125,\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f'shape of train data is {x_train.shape}')\n",
        "    print(f'shape of test data is {x_test.shape}')\n",
        "    print(f'shape of valid data is {x_valid.shape}')\n",
        "    return x_train, x_valid, x_test, y_train, y_valid, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVH6t--9xZq"
      },
      "source": [
        "## Lab 1(b): Implement your function to build a vocabulary based on the training corpus.\n",
        "Implement the build_vocab function to build a vocabulary based on the training corpus.\n",
        "You should first compute the frequency of all the words in the training corpus. Remove the words\n",
        "that are in the STOP_WORDS. Then filter the words by their frequency (≥ min_freq) and finally\n",
        "generate a corpus variable that contains a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sugI5VoJ9xZr"
      },
      "outputs": [],
      "source": [
        "def build_vocab(x_train:list, min_freq: int=5, hparams=None) -> dict:\n",
        "    \"\"\"\n",
        "    build a vocabulary based on the training corpus.\n",
        "    :param x_train:  List. The training corpus. Each sample in the list is a string of text.\n",
        "    :param min_freq: Int. The frequency threshold for selecting words.\n",
        "    :return: dictionary {word:index}\n",
        "    \"\"\"\n",
        "    # Add your code here. Your code should assign corpus with a list of words.\n",
        "    freq = Counter()\n",
        "    for text in x_train:\n",
        "        words = text.split()\n",
        "        freq.update(words)\n",
        "    \n",
        "    corpus = {word:freq[word] for word in freq if word not in hparams.STOP_WORDS}\n",
        "    corpus_ = [word for word, freq in corpus.items() if freq >= min_freq]\n",
        "    # creating a dict\n",
        "    vocab = {w:i+2 for i, w in enumerate(corpus_)}\n",
        "    vocab[hparams.PAD_TOKEN] = hparams.PAD_INDEX\n",
        "    vocab[hparams.UNK_TOKEN] = hparams.UNK_INDEX\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca71G17F9xZt"
      },
      "source": [
        "## Lab 1(c): Implement your tokenize function.\n",
        "For each word, find its index in the vocabulary.\n",
        "Return a list of int that represents the indices of words in the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c6kj_qT69xZt"
      },
      "outputs": [],
      "source": [
        "def tokenize(vocab: dict, example: str)-> list:\n",
        "    \"\"\"\n",
        "    Tokenize the give example string into a list of token indices.\n",
        "    :param vocab: dict, the vocabulary.\n",
        "    :param example: a string of text.\n",
        "    :return: a list of token indices.\n",
        "    \"\"\"\n",
        "    words = example.split()\n",
        "    \n",
        "    # Convert words to indices using vocabulary\n",
        "    indices = [vocab.get(word, vocab[HyperParams().UNK_TOKEN]) for word in words]\n",
        "    \n",
        "    return indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ntSo4k9xZu"
      },
      "source": [
        "## Lab 1 (d): Implement the __getitem__ function. Given an index i, you should return the i-th review and label.\n",
        "The review is originally a string. Please tokenize it into a sequence of token indices.\n",
        "Use the max_length parameter to truncate the sequence so that it contains at most max_length tokens.\n",
        "Convert the label string ('positive'/'negative') to a binary index. 'positive' is 1 and 'negative' is 0.\n",
        "Return a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2TDgA4p79xZu"
      },
      "outputs": [],
      "source": [
        "class IMDB(Dataset):\n",
        "    def __init__(self, x, y, vocab, max_length=256) -> None:\n",
        "        \"\"\"\n",
        "        :param x: list of reviews\n",
        "        :param y: list of labels\n",
        "        :param vocab: vocabulary dictionary {word:index}.\n",
        "        :param max_length: the maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"\n",
        "        Return the tokenized review and label by the given index.\n",
        "        :param idx: index of the sample.\n",
        "        :return: a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label.\n",
        "        \"\"\"\n",
        "        # Add your code here.\n",
        "        review = self.x.iloc[idx]\n",
        "        label = 1 if self.y.iloc[idx] == 'positive' else 0\n",
        "        ids = [self.vocab.get(token, self.vocab[HyperParams().UNK_TOKEN]) for token in review.split()]\n",
        "        length = len(ids)\n",
        "        if length > self.max_length:\n",
        "            ids = ids[:self.max_length]\n",
        "            length = self.max_length  # Update length after truncation\n",
        "        else:\n",
        "            ids = ids + [0] * (self.max_length - length)\n",
        "        \n",
        "        return {'ids': ids, 'length': length, 'label': label}\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = torch.Tensor([i['length'] for i in batch])\n",
        "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
        "    batch = {'ids': batch_ids, 'length': batch_length, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "collate_fn = collate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zgSPYmf9xZv"
      },
      "source": [
        "## Lab 1 (e): Implement the LSTM model for sentiment analysis.\n",
        "Q(a): Implement the initialization function.\n",
        "Your task is to create the model by stacking several necessary layers including an embedding layer, a lstm cell, a linear layer, and a dropout layer.\n",
        "You can call functions from Pytorch's nn library. For example, nn.Embedding, nn.LSTM, nn.Linear.<br>\n",
        "Q(b): Implement the forward function.\n",
        "    Decide where to apply dropout.\n",
        "    The sequences in the batch have different lengths. Write/call a function to pad the sequences into the same length.\n",
        "    Apply a fully-connected (fc) layer to the output of the LSTM layer.\n",
        "    Return the output features which is of size [batch size, output dim]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9ofQ5R29xZv"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Embedding):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        n_layers: int,\n",
        "        dropout_rate: float,\n",
        "        pad_index: int,\n",
        "        bidirectional: bool,\n",
        "        **kwargs):\n",
        "        \"\"\"\n",
        "        Create a LSTM model for classification.\n",
        "        :param vocab_size: size of the vocabulary\n",
        "        :param embedding_dim: dimension of embeddings\n",
        "        :param hidden_dim: dimension of hidden features\n",
        "        :param output_dim: dimension of the output layer which equals to the number of labels.\n",
        "        :param n_layers: number of layers.\n",
        "        :param dropout_rate: dropout rate.\n",
        "        :param pad_index: index of the padding token.we\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Add your code here. Initializing each layer by the given arguments. (you can use nn.LSTM, nn.Embedding, nn.Linear, nn.Dropout)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_rate, bidirectional=bidirectional, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.pad_index = pad_index\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Weight initialization. DO NOT CHANGE!\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        # Ensure lengths don't exceed maximum sequence length\n",
        "        length = torch.clamp(length, max=ids.size(1))\n",
        "        \n",
        "        embedded = self.embedding(ids)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        # Get batch size and seq length for proper shape handling\n",
        "        batch_size, seq_len, embed_dim = embedded.size()\n",
        "        \n",
        "        # LSTM expects [seq_len, batch_size, embed_dim]\n",
        "        embedded = embedded.transpose(0, 1)\n",
        "        \n",
        "        try:\n",
        "            # Pack padded sequence with lengths on same device\n",
        "            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "                embedded, length.to(embedded.device).long(), enforce_sorted=False\n",
        "            )\n",
        "        except RuntimeError:\n",
        "            # Fall back to CPU if needed\n",
        "            lengths_cpu = length.cpu().long()\n",
        "            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "                embedded, lengths_cpu, enforce_sorted=False\n",
        "            )\n",
        "        \n",
        "        # Process through LSTM\n",
        "        _, (hidden, _) = self.lstm(packed_embedded)\n",
        "        \n",
        "        # Get final layer hidden state\n",
        "        final_hidden = hidden[-1]\n",
        "        final_hidden = self.dropout(final_hidden)\n",
        "        \n",
        "        # Linear layer for output\n",
        "        prediction = self.fc(final_hidden)\n",
        "        \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Code (do not modify)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "13Sdl7MV9xZv"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        length = batch['length']\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids, length)\n",
        "        loss = criterion(prediction, label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "        scheduler.step()\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy\n",
        "\n",
        "def predict_sentiment(text, model, vocab, device):\n",
        "    tokens = tokenize(vocab, text)\n",
        "    ids = [vocab[t] if t in vocab else HyperParams().UNK_INDEX for t in tokens]\n",
        "    length = torch.LongTensor([len(ids)])\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor, length).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    return predicted_class, predicted_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnLooBJ4wh_P"
      },
      "source": [
        "### Learning rate warmup. DO NOT TOUCH!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9rHTjuZpwh_P"
      },
      "outputs": [],
      "source": [
        "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        num_warmup_steps: int,\n",
        "    ):\n",
        "        self.num_warmup_steps = num_warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self._step_count <= self.num_warmup_steps:\n",
        "            # warmup\n",
        "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
        "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
        "            self.last_lr = lr\n",
        "        else:\n",
        "            lr = self.base_lrs\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBvNRJWwh_P"
      },
      "source": [
        "### Implement the training / validation iteration here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXLkQSnS9xZw"
      },
      "outputs": [],
      "source": [
        "def train_and_test_model_with_hparams(hparams, model_type=\"lstm\", **kwargs):\n",
        "    # Seeding. DO NOT TOUCH! DO NOT TOUCH hparams.SEED!\n",
        "    # Set the random seeds.\n",
        "    torch.manual_seed(hparams.SEED)\n",
        "    random.seed(hparams.SEED)\n",
        "    np.random.seed(hparams.SEED)\n",
        "\n",
        "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_imdb()\n",
        "    vocab = build_vocab(x_train, hparams=hparams)\n",
        "    vocab_size = len(vocab)\n",
        "    print(f'Length of vocabulary is {vocab_size}')\n",
        "\n",
        "    train_data = IMDB(x_train, y_train, vocab, hparams.MAX_LENGTH)\n",
        "    valid_data = IMDB(x_valid, y_valid, vocab, hparams.MAX_LENGTH)\n",
        "    test_data = IMDB(x_test, y_test, vocab, hparams.MAX_LENGTH)\n",
        "\n",
        "    collate = functools.partial(collate_fn, pad_index=hparams.PAD_INDEX)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate, shuffle=True)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(\n",
        "        valid_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "\n",
        "    # Model\n",
        "\n",
        "    model = LSTM(\n",
        "            vocab_size,\n",
        "            hparams.EMBEDDING_DIM,\n",
        "            hparams.HIDDEN_DIM,\n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE,\n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    \n",
        "    num_params = count_parameters(model)\n",
        "    print(f'The model has {num_params:,} trainable parameters')\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # DO NOT TOUCH optimizer-specific hyperparameters! (e.g., eps, momentum)\n",
        "    # DO NOT change optimizer implementations!\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Start training\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    # Warmup Scheduler. DO NOT TOUCH!\n",
        "    WARMUP_STEPS = 200\n",
        "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
        "\n",
        "    for epoch in range(hparams.N_EPOCHS):\n",
        "        # Your code: implement the training process and save the best model.\n",
        "        train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, lr_scheduler, device)\n",
        "        valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)    \n",
        "\n",
        "        epoch_train_loss = np.mean(train_loss)\n",
        "        epoch_train_acc = np.mean(train_acc)\n",
        "        epoch_valid_loss = np.mean(valid_loss)\n",
        "        epoch_valid_acc = np.mean(valid_acc)\n",
        "\n",
        "        # Save the model that achieves the smallest validation loss.\n",
        "        if epoch_valid_loss < best_valid_loss:\n",
        "            # Your code: save the best model somewhere (no need to submit it to Sakai)\n",
        "            best_valid_loss = epoch_valid_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        print(f'epoch: {epoch+1}')\n",
        "        print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "        print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
        "\n",
        "\n",
        "    # Your Code: Load the best model's weights.\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    # Your Code: evaluate test loss on testing dataset (NOT Validation)\n",
        "    test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
        "    epoch_test_loss = np.mean(test_loss)\n",
        "    epoch_test_acc = np.mean(test_acc)\n",
        "    print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n",
        "    # Your Code: select one of the entries in test set and predict its sentiment, print out the text, prediction and the probability.\n",
        "    index = 0\n",
        "    text = x_test.iloc[index]\n",
        "    label = y_test.iloc[index]\n",
        "    predicted_class, predicted_probability = predict_sentiment(text, model, vocab, device)\n",
        "    print(f'Text: {text}')\n",
        "    print(f'Label: {label}')\n",
        "    print(f'Predicted class: {predicted_class}')\n",
        "    print(f'Predicted probability: {predicted_probability}')\n",
        "\n",
        "    # Free memory for later usage.\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    return {\n",
        "        'num_params': num_params,\n",
        "        \"test_loss\": epoch_test_loss,\n",
        "        \"test_acc\": epoch_test_acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKCu4rPBA2Sp"
      },
      "source": [
        "### Lab 1 (f): Train LSTM model .\n",
        "\n",
        "Train the model with default hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzatRvfMwh_Q",
        "outputId": "330f7065-2980-4791-a834-1779ebec7697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60623\n",
            "The model has 102,025 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [33:31<00:00,  5.51s/it]\n",
            "evaluating...: 100%|██████████| 53/53 [04:44<00:00,  5.38s/it]\n",
            "epoch: 1\n",
            "train_loss: 0.690, train_acc: 0.537\n",
            "valid_loss: 0.666, valid_acc: 0.621\n",
            "training...: 100%|██████████| 365/365 [34:31<00:00,  5.68s/it]\n",
            "evaluating...: 100%|██████████| 53/53 [05:07<00:00,  5.81s/it]\n",
            "epoch: 2\n",
            "train_loss: 0.530, train_acc: 0.751\n",
            "valid_loss: 0.495, valid_acc: 0.757\n",
            "training...: 100%|██████████| 365/365 [36:24<00:00,  5.98s/it]\n",
            "evaluating...: 100%|██████████| 53/53 [05:10<00:00,  5.85s/it]\n",
            "epoch: 3\n",
            "train_loss: 0.389, train_acc: 0.834\n",
            "valid_loss: 0.479, valid_acc: 0.771\n",
            "training...: 100%|██████████| 365/365 [33:34<00:00,  5.52s/it]\n",
            "evaluating...: 100%|██████████| 53/53 [04:02<00:00,  4.57s/it]\n",
            "epoch: 4\n",
            "train_loss: 0.241, train_acc: 0.909\n",
            "valid_loss: 0.431, valid_acc: 0.838\n",
            "training...: 100%|██████████| 365/365 [28:14<00:00,  4.64s/it]\n",
            "evaluating...: 100%|██████████| 53/53 [03:59<00:00,  4.51s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_60328\\884794530.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pt'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 5\n",
            "train_loss: 0.152, train_acc: 0.947\n",
            "valid_loss: 0.448, valid_acc: 0.836\n",
            "evaluating...: 100%|██████████| 105/105 [07:48<00:00,  4.46s/it]\n",
            "test_loss: 0.415, test_acc: 0.840\n",
            "Text: It's hard to tell if Noonan and Marshall are trying to ape Abbott & Costello, Martin & Lewis, Curley & Larry, or some other comedy team, but whoever it was they were trying to imitate, they failed miserably. There's barely one weak laugh in this whole incredibly stupid picture. Noonan (who helped write the alleged \"script\") and Marshall have no chemistry whatsoever; Marshall seems to be trying for a Dean Martin type of devil-may-care coolness, but he doesn't come even remotely close. God knows what Noonan thought he was doing, but being funny sure wasn't it. He seems to think that flapping his arms and legs a lot and staring dumbly at everyone and everything is the height of screen comedy; maybe for him it is, but not for the audience. I remember seeing this in the theaters when it came out. It was on the bottom of a double bill with a three-year-old Jeff Chandler western (\"Pillars of the Sky,\" which was pretty good) and a Three Stooges short (\"Sappy Bullfighters\", which wasn't), and about 20 minutes into this thing all the kids in the audience were throwing stuff at the screen; it was so staggeringly unfunny that it didn't even measure up to the worst of the Three Stooges shorts. I stayed around for the end of it (not that I wanted to, but my folks weren't due to pick me up until after the movie ended), and by the time this mess was over, I was the only one in the theater. I saw it again about 15 or so years ago on cable one night, and stuck around to watch it to see if it was as bad as I remembered. It was worse. The only thing it had going for it was Julie Newmar, who was as smokingly hot as ever; other than her, this thing has absolutely NOTHING to recommend it. The two of them also play Japanese soldiers, and the way they do it makes Jerry Lewis' infamous playing of Japanese characters as thick-lensed, buck-toothed, gibbering mental defectives look benign by comparison. Marshall went on to host \"Hollywood Squares,\" and Noonan kept trying his hand at making movies, but most of them were almost as bad as this. Not quite, though, as I don't think ANYTHING could be quite as bad as this. A truly pathetic waste of film. Don't waste your time on it.\n",
            "Label: negative\n",
            "Predicted class: 0\n",
            "Predicted probability: 0.9177499413490295\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"lstm_1layer_base_adam_e32_h100\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
