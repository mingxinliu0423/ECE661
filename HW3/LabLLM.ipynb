{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune GPT-2 on wiki-text\n",
    "\n",
    "In this Lab, we are using a series of library from Hugging Face (i.e. tranformers, datasets, peft). You may need to go through the document of these library to learn the usage. (Hint: you may use the imported contents in the code cell below, other contents is not necessary for this lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 21.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 36.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 45.3 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, pyyaml, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.1 pyyaml-6.0.2 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.13-cp39-cp39-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp39-cp39-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp39-cp39-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp39-cp39-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp39-cp39-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.13-cp39-cp39-win_amd64.whl (442 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading pyarrow-19.0.1-cp39-cp39-win_amd64.whl (25.5 MB)\n",
      "   ---------------------------------------- 0.0/25.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 4.2/25.5 MB 22.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.0/25.5 MB 25.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.8/25.5 MB 27.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.5 MB 30.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.5/25.5 MB 29.3 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp39-cp39-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.3.0-cp39-cp39-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.18.3-cp39-cp39-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (2.5.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (4.66.5)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from peft) (0.29.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.4.0 peft-0.14.0\n"
     ]
    }
   ],
   "source": [
    "# for google colab\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(a) Generate text with GPT2\n",
    "\n",
    "Using the API provided by hugging face, we can easily load the pre-trained GPT2 model and generate text. (GPT2 is a early generative model, the quality of the generated text is not as good as the later model like GPT3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kojil\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a langugae model based on transformer developed by OpenAI. It is a simple, fast, and scalable model of the human brain. It is based on the concept of the \"brain as a machine\".\n",
      "\n",
      "The model is based on the concept of the \"brain as a machine\". The model is based on the concept of the \"brain as a machine\". The model is based on the concept of the \"brain as a machine\". The model is based on the\n"
     ]
    }
   ],
   "source": [
    "# your code here: load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length):\n",
    "\n",
    "\n",
    "    # your code here: tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # your code here: generate token using the model\n",
    "    gen_tokens = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length)\n",
    "\n",
    "    # your code here: decode the generated tokens\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "generate_text(model, tokenizer, \"GPT-2 is a langugae model based on transformer developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(b) Prepare dataset for training\n",
    "\n",
    "Please fill the code cell below to download the dataset and prepare the dataset for finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3671/3671 [00:00<00:00, 4480.55 examples/s]\n",
      "Map: 100%|██████████| 376/376 [00:00<00:00, 3761.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "DataLoader is working correctly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# your code here: load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# get 10% of dataset\n",
    "dataset_train = dataset[\"train\"].select(range(len(dataset[\"train\"]) // 10))\n",
    "dataset_valid = dataset[\"validation\"].select(range(len(dataset[\"validation\"]) // 10))\n",
    "\n",
    "# your code here: implement function that tokenize the dataset and set labels to be the same as input_ids\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# your code here: tokenize the dataset (you may need to remove columns that are not needed)\n",
    "tokenized_datasets_train = dataset_train.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets_valid = dataset_valid.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_datasets_train.set_format(\"torch\")\n",
    "tokenized_datasets_valid.set_format(\"torch\")\n",
    "\n",
    "# your code here: create datacollator for training and validation dataset\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets_train, shuffle=True, batch_size=4, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(tokenized_datasets_valid, batch_size=4, collate_fn=data_collator)\n",
    "\n",
    "# Test the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break\n",
    "\n",
    "print(\"DataLoader is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(c) Evaluate perplexity on wiki-text\n",
    "\n",
    "Before finetuning, we evaluate the pre-trained GPT2 model on the wiki-text dataset. The perplexity is a common metric to evaluate the performance of language model. The lower the perplexity, the better the model. To compute the perplexity in practice, we use the formula as follows, which is a transformation of the formula in class:\n",
    "$PP(W) = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i|\\text{context})\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial perplexity: 42.995872497558594\n"
     ]
    }
   ],
   "source": [
    "def evaluate_perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # your code here: get the input_ids, attention_mask, and labels from the batch\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # your code here: forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # your code here: calculate the loss\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_length += attention_mask.sum().item()\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_length))\n",
    "    \n",
    "    return perplexity.item()\n",
    "    \n",
    "\n",
    "perplexity = evaluate_perplexity(model, valid_dataloader)\n",
    "print(f\"Initial perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(d) Fine-tune GPT2 on wiki-text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 1:56:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    # your code here: report validation and training loss every epoch\n",
    ")\n",
    "\n",
    "# your code here: create a Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_train,\n",
    "    eval_dataset=tokenized_datasets_valid,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuned perplexity: 27.33197593688965\n"
     ]
    }
   ],
   "source": [
    "# your code here: load the fine-tuned model\n",
    "model_finetuned = AutoModelForCausalLM.from_pretrained(\"./gpt2-wikitext-2\").to(device)\n",
    "perplexity = evaluate_perplexity(model_finetuned, valid_dataloader)\n",
    "print(f\"fine-tuned perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some text using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a langugae model based on transformers developed by OpenAI , and is a novel approach to the synthesis of the functional @-@ type @-@ based protein . The model is based on the interaction between the two nucleotides , and is based on the interaction between the two nucleotides with the substrate . The interaction between the two nucleotides is a fundamental feature of the protein , and is the basis for the synthesis of the functional @-@ type\n"
     ]
    }
   ],
   "source": [
    "# load the fine-tuned model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# generate text\n",
    "generate_text(model_finetuned, tokenizer, \"GPT-2 is a langugae model based on transformers developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(e) Parameter efficient fine-tuning (LoRA)\n",
    "\n",
    "finetune the base gpt model through LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after lora finetuning: 42.995872497558594\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# your code here: load GPT2 model and add the lora adapter\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "model_lora = get_peft_model(model_base, peft_config)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-lora-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# your code here: set trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets_train,\n",
    "    eval_dataset=tokenized_datasets_valid,\n",
    ")\n",
    "\n",
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate lora fine-tuned model on wiki-text\n",
    "\n",
    "compare the text generated by the fully fine-tuned model and LoRA fine-tuned model and the pre-trained model. Do you see any difference in the quality of the generated text? Try to explain why. (Hint: trust your result and report as it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a langugae model based on transformers developed by OpenAI. It is a simple, fast, and scalable model that can be used to generate a large number of models.\n",
      "\n",
      "The model is based on the following principles:\n",
      "\n",
      "The model is based on the following principles:\n",
      "\n",
      "The model is based on the following principles:\n",
      "\n",
      "The model is based on the following principles:\n",
      "\n",
      "The model is based on the following principles:\n",
      "\n",
      "The model\n"
     ]
    }
   ],
   "source": [
    "generate_text(model_lora, tokenizer, \"GPT-2 is a langugae model based on transformers developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the perplexity of the fully fine-tuned model and LoRA fine-tuned model. Do you see any difference in the perplexity? Try to explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after lora finetuning: 42.995872497558594\n"
     ]
    }
   ],
   "source": [
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
