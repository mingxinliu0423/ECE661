{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (a) Model preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from resnet20 import ResNetCIFAR\n",
    "from train_util import train, finetune, test\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from FP_layers import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_14624\\3205097103.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3231, Test accuracy=0.9150\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (b) Prune by percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_percentage(layer, q=70.0):\n",
    "    \"\"\"\n",
    "    Pruning the weight paramters by threshold.\n",
    "    :param q: pruning percentile. 'q' percent of the least \n",
    "    significant weight parameters will be pruned.\n",
    "    \"\"\"\n",
    "    # Convert the weight of \"layer\" to numpy array\n",
    "    layer_weight = layer.weight.data.cpu().numpy()\n",
    "    # Compute the q-th percentile of the abs of the converted array\n",
    "    threshold = np.percentile(np.abs(layer_weight), q)\n",
    "    # Generate a binary mask same shape as weight to decide which element to prune\n",
    "    bi_mask = np.abs(layer_weight) > threshold\n",
    "    # Convert mask to torch tensor and put on GPU\n",
    "    bi_mask = torch.tensor(bi_mask).cuda()\n",
    "    # Multiply the weight by mask to perform pruning\n",
    "    layer.weight.data = layer.weight.data * bi_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.7986111111111112\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.7999674479166666\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of final_fc.linear: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_29456\\2439753093.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=6.3224, Test accuracy=0.1003\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # change q value\n",
    "        prune_by_percentage(layer, q=80.0)\n",
    "        \n",
    "        # Optional: Check the sparsity you achieve in each layer\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        np_weight = layer.weight.data.cpu().numpy()\n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight == 0)\n",
    "        # Count number of parameters\n",
    "        total = np.prod(np_weight.shape)\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (c) Finetune pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_after_prune(net, trainloader, criterion, optimizer, prune=True):\n",
    "    \"\"\"\n",
    "    Finetune the pruned model for a single epoch\n",
    "    Make sure pruned weights are kept as zero\n",
    "    \"\"\"\n",
    "    # Build a dictionary for the nonzero weights\n",
    "    weight_mask = {}\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Your code here: generate a mask in GPU torch tensor to have 1 for nonzero element and 0 for zero element \n",
    "            weight_mask[name] = 1 - (layer.weight.data == 0).float()\n",
    "    \n",
    "    global_steps = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if prune:\n",
    "            for name,layer in net.named_modules():\n",
    "                if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "                    # Your code here: Use weight_mask to make sure zero elements remains zero\n",
    "                    layer.weight.data = layer.weight.data * weight_mask[name]\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        global_steps += 1\n",
    "\n",
    "        if global_steps % 50 == 0:\n",
    "            end = time.time()\n",
    "            batch_size = 256\n",
    "            num_examples_per_second = 50 * batch_size / (end - start)\n",
    "            print(\"[Step=%d]\\tLoss=%.4f\\tacc=%.4f\\t%.1f examples/second\"\n",
    "                 % (global_steps, train_loss / (batch_idx + 1), (correct / total), num_examples_per_second))\n",
    "            start = time.time()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_29456\\93151597.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Get pruned model\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        prune_by_percentage(layer, q=80.0)\n",
    "\n",
    "# Training setup, do not change\n",
    "batch_size=256\n",
    "lr=0.002\n",
    "reg=1e-4\n",
    "\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.875, weight_decay=reg, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.9717\tacc=0.6835\t198.3 examples/second\n",
      "[Step=100]\tLoss=0.8004\tacc=0.7372\t9249.6 examples/second\n",
      "[Step=150]\tLoss=0.7171\tacc=0.7621\t9144.4 examples/second\n",
      "Test Loss=0.5536, Test acc=0.8183\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=50]\tLoss=0.4729\tacc=0.8405\t188.2 examples/second\n",
      "[Step=100]\tLoss=0.4553\tacc=0.8468\t9349.7 examples/second\n",
      "[Step=150]\tLoss=0.4464\tacc=0.8495\t9307.3 examples/second\n",
      "Test Loss=0.4868, Test acc=0.8393\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=50]\tLoss=0.4189\tacc=0.8561\t201.2 examples/second\n",
      "[Step=100]\tLoss=0.4087\tacc=0.8597\t9449.2 examples/second\n",
      "[Step=150]\tLoss=0.3973\tacc=0.8634\t9097.7 examples/second\n",
      "Test Loss=0.4561, Test acc=0.8494\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=50]\tLoss=0.3614\tacc=0.8788\t200.4 examples/second\n",
      "[Step=100]\tLoss=0.3605\tacc=0.8772\t9571.0 examples/second\n",
      "[Step=150]\tLoss=0.3606\tacc=0.8770\t9467.0 examples/second\n",
      "Test Loss=0.4360, Test acc=0.8568\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=50]\tLoss=0.3433\tacc=0.8829\t203.5 examples/second\n",
      "[Step=100]\tLoss=0.3418\tacc=0.8831\t9492.1 examples/second\n",
      "[Step=150]\tLoss=0.3397\tacc=0.8836\t9124.8 examples/second\n",
      "Test Loss=0.4215, Test acc=0.8601\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=50]\tLoss=0.3415\tacc=0.8820\t203.1 examples/second\n",
      "[Step=100]\tLoss=0.3287\tacc=0.8866\t9454.2 examples/second\n",
      "[Step=150]\tLoss=0.3285\tacc=0.8867\t9556.1 examples/second\n",
      "Test Loss=0.4116, Test acc=0.8617\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=50]\tLoss=0.3162\tacc=0.8909\t203.4 examples/second\n",
      "[Step=100]\tLoss=0.3171\tacc=0.8907\t9582.7 examples/second\n",
      "[Step=150]\tLoss=0.3160\tacc=0.8916\t9390.4 examples/second\n",
      "Test Loss=0.4037, Test acc=0.8639\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=50]\tLoss=0.3029\tacc=0.8973\t200.2 examples/second\n",
      "[Step=100]\tLoss=0.3045\tacc=0.8971\t9675.8 examples/second\n",
      "[Step=150]\tLoss=0.3043\tacc=0.8964\t9233.4 examples/second\n",
      "Test Loss=0.3991, Test acc=0.8665\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=50]\tLoss=0.3003\tacc=0.8975\t200.1 examples/second\n",
      "[Step=100]\tLoss=0.2912\tacc=0.9003\t9332.6 examples/second\n",
      "[Step=150]\tLoss=0.2925\tacc=0.8994\t9366.2 examples/second\n",
      "Test Loss=0.3945, Test acc=0.8684\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=50]\tLoss=0.2881\tacc=0.8997\t190.0 examples/second\n",
      "[Step=100]\tLoss=0.2845\tacc=0.9016\t9461.3 examples/second\n",
      "[Step=150]\tLoss=0.2841\tacc=0.9016\t9157.3 examples/second\n",
      "Test Loss=0.3879, Test acc=0.8711\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.2876\tacc=0.9001\t194.5 examples/second\n",
      "[Step=100]\tLoss=0.2837\tacc=0.9017\t8871.5 examples/second\n",
      "[Step=150]\tLoss=0.2842\tacc=0.9014\t8713.6 examples/second\n",
      "Test Loss=0.3875, Test acc=0.8706\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.2766\tacc=0.9045\t182.9 examples/second\n",
      "[Step=100]\tLoss=0.2729\tacc=0.9063\t8948.9 examples/second\n",
      "[Step=150]\tLoss=0.2752\tacc=0.9046\t8889.4 examples/second\n",
      "Test Loss=0.3802, Test acc=0.8722\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.2684\tacc=0.9073\t183.2 examples/second\n",
      "[Step=100]\tLoss=0.2681\tacc=0.9080\t8983.8 examples/second\n",
      "[Step=150]\tLoss=0.2672\tacc=0.9079\t8991.0 examples/second\n",
      "Test Loss=0.3778, Test acc=0.8736\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.2776\tacc=0.9035\t187.4 examples/second\n",
      "[Step=100]\tLoss=0.2689\tacc=0.9070\t9099.1 examples/second\n",
      "[Step=150]\tLoss=0.2686\tacc=0.9079\t9223.5 examples/second\n",
      "Test Loss=0.3758, Test acc=0.8752\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.2623\tacc=0.9086\t190.5 examples/second\n",
      "[Step=100]\tLoss=0.2599\tacc=0.9094\t9004.2 examples/second\n",
      "[Step=150]\tLoss=0.2608\tacc=0.9092\t8959.6 examples/second\n",
      "Test Loss=0.3729, Test acc=0.8756\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.2650\tacc=0.9068\t179.4 examples/second\n",
      "[Step=100]\tLoss=0.2611\tacc=0.9097\t8662.5 examples/second\n",
      "[Step=150]\tLoss=0.2596\tacc=0.9089\t8650.5 examples/second\n",
      "Test Loss=0.3708, Test acc=0.8776\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.2470\tacc=0.9170\t191.5 examples/second\n",
      "[Step=100]\tLoss=0.2470\tacc=0.9161\t8970.9 examples/second\n",
      "[Step=150]\tLoss=0.2477\tacc=0.9153\t8622.8 examples/second\n",
      "Test Loss=0.3693, Test acc=0.8782\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.2467\tacc=0.9138\t202.5 examples/second\n",
      "[Step=100]\tLoss=0.2481\tacc=0.9128\t8840.0 examples/second\n",
      "[Step=150]\tLoss=0.2450\tacc=0.9138\t8499.0 examples/second\n",
      "Test Loss=0.3678, Test acc=0.8782\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.2541\tacc=0.9117\t197.4 examples/second\n",
      "[Step=100]\tLoss=0.2512\tacc=0.9130\t8987.0 examples/second\n",
      "[Step=150]\tLoss=0.2471\tacc=0.9138\t8777.2 examples/second\n",
      "Test Loss=0.3669, Test acc=0.8786\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.2287\tacc=0.9213\t198.2 examples/second\n",
      "[Step=100]\tLoss=0.2369\tacc=0.9177\t8484.9 examples/second\n",
      "[Step=150]\tLoss=0.2415\tacc=0.9158\t8639.9 examples/second\n",
      "Test Loss=0.3652, Test acc=0.8784\n"
     ]
    }
   ],
   "source": [
    "# Model finetuning\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        print(\"Saving...\")\n",
    "        torch.save(net.state_dict(), \"net_after_finetune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.7986111111111112\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.7999674479166666\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of final_fc.linear: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_29456\\2640758429.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"net_after_finetune.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3669, Test accuracy=0.8786\n"
     ]
    }
   ],
   "source": [
    "# Check sparsity of the finetuned model, make sure it's not changed\n",
    "net.load_state_dict(torch.load(\"net_after_finetune.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here:\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        np_weight = layer.weight.data.cpu().numpy()\n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight == 0)\n",
    "        # Count number of parameters\n",
    "        total = np.prod(np_weight.shape)\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (d) Iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Pruning head_conv.0.conv with q=8\n",
      "Pruning body_op.0.conv1.0.conv with q=8\n",
      "Pruning body_op.0.conv2.0.conv with q=8\n",
      "Pruning body_op.1.conv1.0.conv with q=8\n",
      "Pruning body_op.1.conv2.0.conv with q=8\n",
      "Pruning body_op.2.conv1.0.conv with q=8\n",
      "Pruning body_op.2.conv2.0.conv with q=8\n",
      "Pruning body_op.3.conv1.0.conv with q=8\n",
      "Pruning body_op.3.conv2.0.conv with q=8\n",
      "Pruning body_op.4.conv1.0.conv with q=8\n",
      "Pruning body_op.4.conv2.0.conv with q=8\n",
      "Pruning body_op.5.conv1.0.conv with q=8\n",
      "Pruning body_op.5.conv2.0.conv with q=8\n",
      "Pruning body_op.6.conv1.0.conv with q=8\n",
      "Pruning body_op.6.conv2.0.conv with q=8\n",
      "Pruning body_op.7.conv1.0.conv with q=8\n",
      "Pruning body_op.7.conv2.0.conv with q=8\n",
      "Pruning body_op.8.conv1.0.conv with q=8\n",
      "Pruning body_op.8.conv2.0.conv with q=8\n",
      "Pruning final_fc.linear with q=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_29456\\1262236627.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step=50]\tLoss=0.0490\tacc=0.9844\t201.4 examples/second\n",
      "[Step=100]\tLoss=0.0486\tacc=0.9848\t8592.1 examples/second\n",
      "[Step=150]\tLoss=0.0479\tacc=0.9848\t8450.6 examples/second\n",
      "Test Loss=0.3246, Test acc=0.9149\n",
      "\n",
      "Epoch: 1\n",
      "Pruning head_conv.0.conv with q=16\n",
      "Pruning body_op.0.conv1.0.conv with q=16\n",
      "Pruning body_op.0.conv2.0.conv with q=16\n",
      "Pruning body_op.1.conv1.0.conv with q=16\n",
      "Pruning body_op.1.conv2.0.conv with q=16\n",
      "Pruning body_op.2.conv1.0.conv with q=16\n",
      "Pruning body_op.2.conv2.0.conv with q=16\n",
      "Pruning body_op.3.conv1.0.conv with q=16\n",
      "Pruning body_op.3.conv2.0.conv with q=16\n",
      "Pruning body_op.4.conv1.0.conv with q=16\n",
      "Pruning body_op.4.conv2.0.conv with q=16\n",
      "Pruning body_op.5.conv1.0.conv with q=16\n",
      "Pruning body_op.5.conv2.0.conv with q=16\n",
      "Pruning body_op.6.conv1.0.conv with q=16\n",
      "Pruning body_op.6.conv2.0.conv with q=16\n",
      "Pruning body_op.7.conv1.0.conv with q=16\n",
      "Pruning body_op.7.conv2.0.conv with q=16\n",
      "Pruning body_op.8.conv1.0.conv with q=16\n",
      "Pruning body_op.8.conv2.0.conv with q=16\n",
      "Pruning final_fc.linear with q=16\n",
      "[Step=50]\tLoss=0.0508\tacc=0.9841\t203.5 examples/second\n",
      "[Step=100]\tLoss=0.0505\tacc=0.9839\t8810.9 examples/second\n",
      "[Step=150]\tLoss=0.0494\tacc=0.9840\t8775.2 examples/second\n",
      "Test Loss=0.3255, Test acc=0.9156\n",
      "\n",
      "Epoch: 2\n",
      "Pruning head_conv.0.conv with q=24\n",
      "Pruning body_op.0.conv1.0.conv with q=24\n",
      "Pruning body_op.0.conv2.0.conv with q=24\n",
      "Pruning body_op.1.conv1.0.conv with q=24\n",
      "Pruning body_op.1.conv2.0.conv with q=24\n",
      "Pruning body_op.2.conv1.0.conv with q=24\n",
      "Pruning body_op.2.conv2.0.conv with q=24\n",
      "Pruning body_op.3.conv1.0.conv with q=24\n",
      "Pruning body_op.3.conv2.0.conv with q=24\n",
      "Pruning body_op.4.conv1.0.conv with q=24\n",
      "Pruning body_op.4.conv2.0.conv with q=24\n",
      "Pruning body_op.5.conv1.0.conv with q=24\n",
      "Pruning body_op.5.conv2.0.conv with q=24\n",
      "Pruning body_op.6.conv1.0.conv with q=24\n",
      "Pruning body_op.6.conv2.0.conv with q=24\n",
      "Pruning body_op.7.conv1.0.conv with q=24\n",
      "Pruning body_op.7.conv2.0.conv with q=24\n",
      "Pruning body_op.8.conv1.0.conv with q=24\n",
      "Pruning body_op.8.conv2.0.conv with q=24\n",
      "Pruning final_fc.linear with q=24\n",
      "[Step=50]\tLoss=0.0523\tacc=0.9829\t201.8 examples/second\n",
      "[Step=100]\tLoss=0.0502\tacc=0.9839\t8768.5 examples/second\n",
      "[Step=150]\tLoss=0.0500\tacc=0.9843\t8905.8 examples/second\n",
      "Test Loss=0.3301, Test acc=0.9128\n",
      "\n",
      "Epoch: 3\n",
      "Pruning head_conv.0.conv with q=32\n",
      "Pruning body_op.0.conv1.0.conv with q=32\n",
      "Pruning body_op.0.conv2.0.conv with q=32\n",
      "Pruning body_op.1.conv1.0.conv with q=32\n",
      "Pruning body_op.1.conv2.0.conv with q=32\n",
      "Pruning body_op.2.conv1.0.conv with q=32\n",
      "Pruning body_op.2.conv2.0.conv with q=32\n",
      "Pruning body_op.3.conv1.0.conv with q=32\n",
      "Pruning body_op.3.conv2.0.conv with q=32\n",
      "Pruning body_op.4.conv1.0.conv with q=32\n",
      "Pruning body_op.4.conv2.0.conv with q=32\n",
      "Pruning body_op.5.conv1.0.conv with q=32\n",
      "Pruning body_op.5.conv2.0.conv with q=32\n",
      "Pruning body_op.6.conv1.0.conv with q=32\n",
      "Pruning body_op.6.conv2.0.conv with q=32\n",
      "Pruning body_op.7.conv1.0.conv with q=32\n",
      "Pruning body_op.7.conv2.0.conv with q=32\n",
      "Pruning body_op.8.conv1.0.conv with q=32\n",
      "Pruning body_op.8.conv2.0.conv with q=32\n",
      "Pruning final_fc.linear with q=32\n",
      "[Step=50]\tLoss=0.0585\tacc=0.9809\t203.1 examples/second\n",
      "[Step=100]\tLoss=0.0562\tacc=0.9818\t8879.0 examples/second\n",
      "[Step=150]\tLoss=0.0561\tacc=0.9817\t8939.8 examples/second\n",
      "Test Loss=0.3319, Test acc=0.9127\n",
      "\n",
      "Epoch: 4\n",
      "Pruning head_conv.0.conv with q=40\n",
      "Pruning body_op.0.conv1.0.conv with q=40\n",
      "Pruning body_op.0.conv2.0.conv with q=40\n",
      "Pruning body_op.1.conv1.0.conv with q=40\n",
      "Pruning body_op.1.conv2.0.conv with q=40\n",
      "Pruning body_op.2.conv1.0.conv with q=40\n",
      "Pruning body_op.2.conv2.0.conv with q=40\n",
      "Pruning body_op.3.conv1.0.conv with q=40\n",
      "Pruning body_op.3.conv2.0.conv with q=40\n",
      "Pruning body_op.4.conv1.0.conv with q=40\n",
      "Pruning body_op.4.conv2.0.conv with q=40\n",
      "Pruning body_op.5.conv1.0.conv with q=40\n",
      "Pruning body_op.5.conv2.0.conv with q=40\n",
      "Pruning body_op.6.conv1.0.conv with q=40\n",
      "Pruning body_op.6.conv2.0.conv with q=40\n",
      "Pruning body_op.7.conv1.0.conv with q=40\n",
      "Pruning body_op.7.conv2.0.conv with q=40\n",
      "Pruning body_op.8.conv1.0.conv with q=40\n",
      "Pruning body_op.8.conv2.0.conv with q=40\n",
      "Pruning final_fc.linear with q=40\n",
      "[Step=50]\tLoss=0.0694\tacc=0.9759\t201.3 examples/second\n",
      "[Step=100]\tLoss=0.0675\tacc=0.9771\t8691.7 examples/second\n",
      "[Step=150]\tLoss=0.0668\tacc=0.9780\t8550.2 examples/second\n",
      "Test Loss=0.3366, Test acc=0.9098\n",
      "\n",
      "Epoch: 5\n",
      "Pruning head_conv.0.conv with q=48\n",
      "Pruning body_op.0.conv1.0.conv with q=48\n",
      "Pruning body_op.0.conv2.0.conv with q=48\n",
      "Pruning body_op.1.conv1.0.conv with q=48\n",
      "Pruning body_op.1.conv2.0.conv with q=48\n",
      "Pruning body_op.2.conv1.0.conv with q=48\n",
      "Pruning body_op.2.conv2.0.conv with q=48\n",
      "Pruning body_op.3.conv1.0.conv with q=48\n",
      "Pruning body_op.3.conv2.0.conv with q=48\n",
      "Pruning body_op.4.conv1.0.conv with q=48\n",
      "Pruning body_op.4.conv2.0.conv with q=48\n",
      "Pruning body_op.5.conv1.0.conv with q=48\n",
      "Pruning body_op.5.conv2.0.conv with q=48\n",
      "Pruning body_op.6.conv1.0.conv with q=48\n",
      "Pruning body_op.6.conv2.0.conv with q=48\n",
      "Pruning body_op.7.conv1.0.conv with q=48\n",
      "Pruning body_op.7.conv2.0.conv with q=48\n",
      "Pruning body_op.8.conv1.0.conv with q=48\n",
      "Pruning body_op.8.conv2.0.conv with q=48\n",
      "Pruning final_fc.linear with q=48\n",
      "[Step=50]\tLoss=0.0873\tacc=0.9700\t203.7 examples/second\n",
      "[Step=100]\tLoss=0.0845\tacc=0.9712\t9111.6 examples/second\n",
      "[Step=150]\tLoss=0.0851\tacc=0.9712\t9013.8 examples/second\n",
      "Test Loss=0.3375, Test acc=0.9070\n",
      "\n",
      "Epoch: 6\n",
      "Pruning head_conv.0.conv with q=56\n",
      "Pruning body_op.0.conv1.0.conv with q=56\n",
      "Pruning body_op.0.conv2.0.conv with q=56\n",
      "Pruning body_op.1.conv1.0.conv with q=56\n",
      "Pruning body_op.1.conv2.0.conv with q=56\n",
      "Pruning body_op.2.conv1.0.conv with q=56\n",
      "Pruning body_op.2.conv2.0.conv with q=56\n",
      "Pruning body_op.3.conv1.0.conv with q=56\n",
      "Pruning body_op.3.conv2.0.conv with q=56\n",
      "Pruning body_op.4.conv1.0.conv with q=56\n",
      "Pruning body_op.4.conv2.0.conv with q=56\n",
      "Pruning body_op.5.conv1.0.conv with q=56\n",
      "Pruning body_op.5.conv2.0.conv with q=56\n",
      "Pruning body_op.6.conv1.0.conv with q=56\n",
      "Pruning body_op.6.conv2.0.conv with q=56\n",
      "Pruning body_op.7.conv1.0.conv with q=56\n",
      "Pruning body_op.7.conv2.0.conv with q=56\n",
      "Pruning body_op.8.conv1.0.conv with q=56\n",
      "Pruning body_op.8.conv2.0.conv with q=56\n",
      "Pruning final_fc.linear with q=56\n",
      "[Step=50]\tLoss=0.1449\tacc=0.9488\t197.5 examples/second\n",
      "[Step=100]\tLoss=0.1298\tacc=0.9544\t8812.9 examples/second\n",
      "[Step=150]\tLoss=0.1236\tacc=0.9571\t8922.9 examples/second\n",
      "Test Loss=0.3391, Test acc=0.9008\n",
      "\n",
      "Epoch: 7\n",
      "Pruning head_conv.0.conv with q=64\n",
      "Pruning body_op.0.conv1.0.conv with q=64\n",
      "Pruning body_op.0.conv2.0.conv with q=64\n",
      "Pruning body_op.1.conv1.0.conv with q=64\n",
      "Pruning body_op.1.conv2.0.conv with q=64\n",
      "Pruning body_op.2.conv1.0.conv with q=64\n",
      "Pruning body_op.2.conv2.0.conv with q=64\n",
      "Pruning body_op.3.conv1.0.conv with q=64\n",
      "Pruning body_op.3.conv2.0.conv with q=64\n",
      "Pruning body_op.4.conv1.0.conv with q=64\n",
      "Pruning body_op.4.conv2.0.conv with q=64\n",
      "Pruning body_op.5.conv1.0.conv with q=64\n",
      "Pruning body_op.5.conv2.0.conv with q=64\n",
      "Pruning body_op.6.conv1.0.conv with q=64\n",
      "Pruning body_op.6.conv2.0.conv with q=64\n",
      "Pruning body_op.7.conv1.0.conv with q=64\n",
      "Pruning body_op.7.conv2.0.conv with q=64\n",
      "Pruning body_op.8.conv1.0.conv with q=64\n",
      "Pruning body_op.8.conv2.0.conv with q=64\n",
      "Pruning final_fc.linear with q=64\n",
      "[Step=50]\tLoss=0.1800\tacc=0.9387\t203.6 examples/second\n",
      "[Step=100]\tLoss=0.1733\tacc=0.9405\t8763.6 examples/second\n",
      "[Step=150]\tLoss=0.1683\tacc=0.9415\t8731.7 examples/second\n",
      "Test Loss=0.3507, Test acc=0.8907\n",
      "\n",
      "Epoch: 8\n",
      "Pruning head_conv.0.conv with q=72\n",
      "Pruning body_op.0.conv1.0.conv with q=72\n",
      "Pruning body_op.0.conv2.0.conv with q=72\n",
      "Pruning body_op.1.conv1.0.conv with q=72\n",
      "Pruning body_op.1.conv2.0.conv with q=72\n",
      "Pruning body_op.2.conv1.0.conv with q=72\n",
      "Pruning body_op.2.conv2.0.conv with q=72\n",
      "Pruning body_op.3.conv1.0.conv with q=72\n",
      "Pruning body_op.3.conv2.0.conv with q=72\n",
      "Pruning body_op.4.conv1.0.conv with q=72\n",
      "Pruning body_op.4.conv2.0.conv with q=72\n",
      "Pruning body_op.5.conv1.0.conv with q=72\n",
      "Pruning body_op.5.conv2.0.conv with q=72\n",
      "Pruning body_op.6.conv1.0.conv with q=72\n",
      "Pruning body_op.6.conv2.0.conv with q=72\n",
      "Pruning body_op.7.conv1.0.conv with q=72\n",
      "Pruning body_op.7.conv2.0.conv with q=72\n",
      "Pruning body_op.8.conv1.0.conv with q=72\n",
      "Pruning body_op.8.conv2.0.conv with q=72\n",
      "Pruning final_fc.linear with q=72\n",
      "[Step=50]\tLoss=0.2908\tacc=0.8988\t199.8 examples/second\n",
      "[Step=100]\tLoss=0.2631\tacc=0.9077\t8549.2 examples/second\n",
      "[Step=150]\tLoss=0.2505\tacc=0.9124\t8996.0 examples/second\n",
      "Test Loss=0.3711, Test acc=0.8803\n",
      "\n",
      "Epoch: 9\n",
      "Pruning head_conv.0.conv with q=80\n",
      "Pruning body_op.0.conv1.0.conv with q=80\n",
      "Pruning body_op.0.conv2.0.conv with q=80\n",
      "Pruning body_op.1.conv1.0.conv with q=80\n",
      "Pruning body_op.1.conv2.0.conv with q=80\n",
      "Pruning body_op.2.conv1.0.conv with q=80\n",
      "Pruning body_op.2.conv2.0.conv with q=80\n",
      "Pruning body_op.3.conv1.0.conv with q=80\n",
      "Pruning body_op.3.conv2.0.conv with q=80\n",
      "Pruning body_op.4.conv1.0.conv with q=80\n",
      "Pruning body_op.4.conv2.0.conv with q=80\n",
      "Pruning body_op.5.conv1.0.conv with q=80\n",
      "Pruning body_op.5.conv2.0.conv with q=80\n",
      "Pruning body_op.6.conv1.0.conv with q=80\n",
      "Pruning body_op.6.conv2.0.conv with q=80\n",
      "Pruning body_op.7.conv1.0.conv with q=80\n",
      "Pruning body_op.7.conv2.0.conv with q=80\n",
      "Pruning body_op.8.conv1.0.conv with q=80\n",
      "Pruning body_op.8.conv2.0.conv with q=80\n",
      "Pruning final_fc.linear with q=80\n",
      "[Step=50]\tLoss=0.5711\tacc=0.8080\t200.8 examples/second\n",
      "[Step=100]\tLoss=0.5124\tacc=0.8279\t8510.2 examples/second\n",
      "[Step=150]\tLoss=0.4801\tacc=0.8371\t8443.8 examples/second\n",
      "Test Loss=0.4763, Test acc=0.8446\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.3823\tacc=0.8667\t186.7 examples/second\n",
      "[Step=100]\tLoss=0.3776\tacc=0.8693\t8840.0 examples/second\n",
      "[Step=150]\tLoss=0.3727\tacc=0.8713\t8611.9 examples/second\n",
      "Test Loss=0.4414, Test acc=0.8561\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.3429\tacc=0.8816\t187.3 examples/second\n",
      "[Step=100]\tLoss=0.3440\tacc=0.8830\t8709.9 examples/second\n",
      "[Step=150]\tLoss=0.3403\tacc=0.8841\t8707.9 examples/second\n",
      "Test Loss=0.4228, Test acc=0.8600\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.3156\tacc=0.8933\t192.3 examples/second\n",
      "[Step=100]\tLoss=0.3187\tacc=0.8912\t8715.7 examples/second\n",
      "[Step=150]\tLoss=0.3183\tacc=0.8918\t8473.2 examples/second\n",
      "Test Loss=0.4117, Test acc=0.8657\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.3013\tacc=0.8967\t198.0 examples/second\n",
      "[Step=100]\tLoss=0.3020\tacc=0.8970\t8591.7 examples/second\n",
      "[Step=150]\tLoss=0.3018\tacc=0.8956\t8812.9 examples/second\n",
      "Test Loss=0.4031, Test acc=0.8667\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.2983\tacc=0.8972\t202.9 examples/second\n",
      "[Step=100]\tLoss=0.2918\tacc=0.9002\t8634.6 examples/second\n",
      "[Step=150]\tLoss=0.2935\tacc=0.8998\t8880.1 examples/second\n",
      "Test Loss=0.3948, Test acc=0.8688\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.2886\tacc=0.9001\t202.7 examples/second\n",
      "[Step=100]\tLoss=0.2838\tacc=0.9026\t8519.7 examples/second\n",
      "[Step=150]\tLoss=0.2848\tacc=0.9018\t8554.2 examples/second\n",
      "Test Loss=0.3892, Test acc=0.8711\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.2705\tacc=0.9080\t200.6 examples/second\n",
      "[Step=100]\tLoss=0.2742\tacc=0.9062\t8685.1 examples/second\n",
      "[Step=150]\tLoss=0.2777\tacc=0.9045\t8843.5 examples/second\n",
      "Test Loss=0.3846, Test acc=0.8719\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.2804\tacc=0.9024\t201.4 examples/second\n",
      "[Step=100]\tLoss=0.2735\tacc=0.9050\t8447.3 examples/second\n",
      "[Step=150]\tLoss=0.2746\tacc=0.9048\t8543.7 examples/second\n",
      "Test Loss=0.3818, Test acc=0.8727\n",
      "Saving...\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.2749\tacc=0.9038\t202.6 examples/second\n",
      "[Step=100]\tLoss=0.2722\tacc=0.9051\t8595.0 examples/second\n",
      "[Step=150]\tLoss=0.2677\tacc=0.9068\t8591.6 examples/second\n",
      "Test Loss=0.3798, Test acc=0.8749\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.2691\tacc=0.9064\t199.9 examples/second\n",
      "[Step=100]\tLoss=0.2641\tacc=0.9092\t8851.0 examples/second\n",
      "[Step=150]\tLoss=0.2634\tacc=0.9093\t8604.3 examples/second\n",
      "Test Loss=0.3765, Test acc=0.8758\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "best_acc = 0.\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    net.train()\n",
    "    if epoch<10:\n",
    "        for name,layer in net.named_modules():\n",
    "            if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "                # Increase model sparsity\n",
    "                q = 8*(epoch+1)\n",
    "                print(\"Pruning \"+name+\" with q=\"+str(q))\n",
    "                prune_by_percentage(layer, q=q)\n",
    "    if epoch<9:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n",
    "    else:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    \n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    \n",
    "    if epoch>=10:\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"net_after_iterative_prune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.7986111111111112\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.7999131944444444\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.7999674479166666\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7999945746527778\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.7999945746527778\n",
      "Sparsity of final_fc.linear: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_29456\\2153638618.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"net_after_iterative_prune.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3765, Test accuracy=0.8758\n"
     ]
    }
   ],
   "source": [
    "# Check sparsity of the final model, make sure it's 80%\n",
    "net.load_state_dict(torch.load(\"net_after_iterative_prune.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        np_weight = layer.weight.data.cpu().numpy()\n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight == 0)\n",
    "        # Count number of parameters\n",
    "        total = np.prod(np_weight.shape)\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (e) Global iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_prune_by_percentage(net, q=70.0):\n",
    "    \"\"\"\n",
    "    Pruning the weight paramters by threshold.\n",
    "    :param q: pruning percentile. 'q' percent of the least \n",
    "    significant weight parameters will be pruned.\n",
    "    \"\"\"\n",
    "    # A list to gather all the weights\n",
    "    flattened_weights = []\n",
    "    # Find global pruning threshold\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Convert weight to numpy\n",
    "            layer_weight = layer.weight.data.cpu().numpy()\n",
    "            # Flatten the weight and append to flattened_weights\n",
    "            flattened_weights.append(layer_weight.flatten())\n",
    "    # Concate all weights into a np array\n",
    "    flattened_weights = np.concatenate(flattened_weights)\n",
    "    # Find global pruning threshold\n",
    "    thres = np.percentile(np.abs(flattened_weights), q)\n",
    "    print(\"Global pruning threshold (q=%d): %.4f\" % (q, thres))\n",
    "    \n",
    "    # Apply pruning threshold to all layers\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Convert weight to numpy\n",
    "            layer_weight = layer.weight.data.cpu().numpy()\n",
    "            # Generate a binary mask same shape as weight to decide which element to prune\n",
    "            bi_mask = np.abs(layer_weight) > thres\n",
    "            # Convert mask to torch tensor and put on GPU\n",
    "            bi_mask = torch.tensor(bi_mask).cuda()\n",
    "            # Multiply the weight by mask to perform pruning\n",
    "            layer.weight.data = layer.weight.data * bi_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_14624\\1584474449.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Global pruning threshold (q=8): 0.0085\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'finetune_after_prune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     global_prune_by_percentage(net, q\u001b[38;5;241m=\u001b[39mq)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m9\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mfinetune_after_prune\u001b[49m(net, trainloader, criterion, optimizer,prune\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     finetune_after_prune(net, trainloader, criterion, optimizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'finetune_after_prune' is not defined"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "best_acc = 0.\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    net.train()\n",
    "    if epoch<10:\n",
    "        # Increase model sparsity\n",
    "        q = 8*(epoch+1)\n",
    "        global_prune_by_percentage(net, q=q)\n",
    "    if epoch<9:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n",
    "    else:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    \n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    \n",
    "    if epoch>=10:\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"net_after_global_iterative_prune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_14624\\273133103.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.3101851851851852\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.6584201388888888\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.6397569444444444\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.6271701388888888\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.6475694444444444\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.6310763888888888\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.6688368055555556\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6241319444444444\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6872829861111112\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.7260199652777778\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.7825520833333334\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.7250434027777778\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.8135850694444444\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.7328559027777778\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7647026909722222\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7768283420138888\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.8259819878472222\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.8525661892361112\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.9767252604166666\n",
      "Sparsity of final_fc.linear: 0.15625\n",
      "Total sparsity of: 0.8000007453342078\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3433, Test accuracy=0.8852\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "\n",
    "zeros_sum = 0\n",
    "total_sum = 0\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here:\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        np_weight = layer.weight.data.cpu().numpy()\n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight == 0)\n",
    "        # Count number of parameters\n",
    "        total = np.prod(np_weight.shape)\n",
    "        zeros_sum+=zeros\n",
    "        total_sum+=total\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "print('Total sparsity of: '+str(zeros_sum/total_sum))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3 (b) and (c): Fixed-point quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kojil\\AppData\\Local\\Temp\\ipykernel_29456\\898108629.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001D186080A60>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'ellipsis' and 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kojil\\Desktop\\ECE661\\HW4\\train_util.py:218\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[0;32m    217\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 218\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    221\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kojil\\Desktop\\ECE661\\HW4\\resnet20.py:79\u001b[0m, in \u001b[0;36mResNetCIFAR.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     78\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_conv(x)\n\u001b[1;32m---> 79\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_pool(out)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_1d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kojil\\Desktop\\ECE661\\HW4\\resnet20.py:30\u001b[0m, in \u001b[0;36mResNet_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kojil\\Desktop\\ECE661\\HW4\\FP_layers.py:88\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m=\u001b[39m stride\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(in_channels, out_channels, kernel_size, stride, padding, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNbits \u001b[38;5;241m=\u001b[39m Nbits\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymmetric \u001b[38;5;241m=\u001b[39m symmetric\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Initialization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kojil\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kojil\\Desktop\\ECE661\\HW4\\FP_layers.py:43\u001b[0m, in \u001b[0;36mSTE.forward\u001b[1;34m(ctx, w, bit, symmetric)\u001b[0m\n\u001b[0;32m     41\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(w))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Scale w to [-1, 1] range\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m ws \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m/\u001b[39m alpha\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# For b-bit symmetric quantization\u001b[39;00m\n\u001b[0;32m     46\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (bit \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'ellipsis' and 'ellipsis'"
     ]
    }
   ],
   "source": [
    "# Define quantized model and load weight\n",
    "Nbits = 6 #Change this value to finish (b) and (c)\n",
    "\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized model finetuning\n",
    "finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)   \n",
    "\n",
    "# Load the model with best accuracy\n",
    "net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3 (d) Quantize pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantized model and load weight\n",
    "Nbits = 3 #Change this value to finish (d)\n",
    "\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized model finetuning\n",
    "finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n",
    "\n",
    "# Load the model with best accuracy\n",
    "net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab4 (a) Symmetric quantization\n",
    "#### Implement symmetric quantization in FP_layers.py, and repeat the process in (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the performance of symmetric quantization with 6, 5, 4, 3, 2 bits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
